{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05d882f",
   "metadata": {},
   "source": [
    "# Consumer 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763eef7e",
   "metadata": {},
   "source": [
    "https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch04.html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b245c4b",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d1580e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('json-changes-event-consumer')\n",
    "         # Add kafka package\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,mysql:mysql-connector-java:8.0.11,com.datastax.spark:spark-cassandra-connector_2.12:3.0.1\")\n",
    "         .config(\"spark.cassandra.connection.host\",\"185.185.126.143\")\n",
    "         .config(\"spark.cassandra.connection.port\",\"9742\")\n",
    "         .config(\"spark.cassandra.auth.username\",\"cassandra\")\n",
    "         .config(\"spark.cassandra.auth.password\",\"cassandra\")\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784fbb39",
   "metadata": {},
   "source": [
    "# Select if you want to run on SMALL OR LARGE DATA SET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd025d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stream dataframe setting kafka server, topic and offset option FOR SMALL DATA SET \n",
    "df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"185.185.126.143:9092\") # kafka server\n",
    "  .option(\"subscribe\", \"S_TOPIC\") # topic name matching the producer \n",
    "  .option(\"startingOffsets\", \"latest\") # start from beginning select  \"latest\" or earliest\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a5201a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stream dataframe setting kafka server, topic and offset option FOR LARGE DATA SET \n",
    "df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"185.185.126.143:9092\") # kafka server\n",
    "  .option(\"subscribe\", \"S2_TOPIC\") # topic name matching the producer \n",
    "  .option(\"startingOffsets\", \"latest\") # start from beginning select  \"latest\" or earliest\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eccf1f",
   "metadata": {},
   "source": [
    "# Parse the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac1bed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Convert binary to string key and value\n",
    "df1 = (df\n",
    "    .withColumn(\"key\", df[\"key\"])\n",
    "    .withColumn(\"value\", df[\"value\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d98a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stream dataframe setting kafka server, topic and offset option\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType() \\\n",
    "        .add(\"review_id\", StringType()) \\\n",
    "        .add(\"user_id\", StringType()) \\\n",
    "        .add(\"business_id\", StringType()) \\\n",
    "        .add(\"stars\", FloatType()) \\\n",
    "        .add(\"Date_Only\", DateType()) \\\n",
    "        .add(\"Time_Only\", StringType()) \\\n",
    "        .add(\"Date_Hour\", StringType()) \\\n",
    "        .add(\"Date_Minute\", StringType()) \\\n",
    "        .add(\"Date_Time\", TimestampType()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "def0d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##option 1 \n",
    "df2=df\\\n",
    "      .selectExpr(\"split(value,',')[0] as review_id\" \\\n",
    "                  ,\"split(value,',')[1] as business_id\" \\\n",
    "                  ,\"split(value,',')[2] as user_id\" \\\n",
    "                  ,\"split(value,',')[3] as stars\" \\\n",
    "                  ,\"split(value,',')[4] as date_only\" \\\n",
    "                  ,\"split(value,',')[5] as time_only\" \\\n",
    "                  ,\"split(value,',')[6] as date_hour\" \\\n",
    "                  ,\"split(value,',')[7] as date_minute\" \\\n",
    "                  ,\"split(value,',')[8] as date_time\" \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b01ca4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "##option 2 \n",
    "df2= df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ad8eb",
   "metadata": {},
   "source": [
    "df2.writeStream.trigger(processingTime='10 seconds').outputMode(\"append\") \\\n",
    "            .format(\"console\") \\\n",
    "            .option(\"checkpointLocation\", \"/home/jovyan/work/Data/cp\")\\\n",
    "            .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0e52c5",
   "metadata": {},
   "source": [
    "https://www.learntospark.com/2020/01/cast-string-to-timestamp-in-spark.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "240cfb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- stars: string (nullable = true)\n",
      " |-- date_only: string (nullable = true)\n",
      " |-- time_only: string (nullable = true)\n",
      " |-- date_hour: string (nullable = true)\n",
      " |-- date_minute: string (nullable = true)\n",
      " |-- date_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "df2=df2.withColumn(\"date_time\",unix_timestamp(\"date_time\", 'MM/dd/yyyy HH:mm:ss').cast(TimestampType()))\n",
    "df2.printSchema()\n",
    "#df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c530a9a3",
   "metadata": {},
   "source": [
    "# Set up folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea7e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set local locations \n",
    "raw_path = \"/home/jovyan/work/Data/detail\"\n",
    "checkpoint_path = \"/home/jovyan/work/Data/cp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f5646",
   "metadata": {},
   "source": [
    "df2.writeStream.outputMode(\"append\") \\\n",
    "            .format(\"console\") \\\n",
    "            .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf90e9",
   "metadata": {},
   "source": [
    "## 1. All data to Parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86d8f134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Save the output to disk in Parquet files\n",
    "# Output is lowest grain with selected columns , less text. \n",
    "# What is the speed this is written to here ??\n",
    "#partition by 1 column and then write to disk \n",
    "\n",
    "\n",
    "raw_path = \"/home/jovyan/work/Data/detail\"\n",
    "checkpoint_path = \"/home/jovyan/work/Data/cp\"\n",
    "\n",
    "\n",
    "queryStream =(\n",
    "     df2.repartition(1).writeStream.partitionBy('date_only')  \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .format(\"parquet\") \\\n",
    "    .queryName(\"base3\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\\\n",
    "    .option(\"path\", raw_path)\\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc815a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional : Read parquet files as stream to output the number of rows (could use for rest of this code??)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c8178b",
   "metadata": {},
   "source": [
    "## 2. Summary Table to Cassandra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e09a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.withWatermark(\"Date_Time\", \"1 minutes\") \\\n",
    "#.withColumn('timestamp', unix_timestamp(col('Date_Time'), \"MM/dd/yyyy hh:mm:ss aa\").cast(TimestampType()))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5345ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b03358a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing/Data Transformation time stamp partition key \n",
    "event_message_detail_agg_df = df2 \\\n",
    "        .withColumn('timestamp', to_timestamp(current_timestamp(),\"MM-dd-yyyy HH mm ss SSS\"))\\\n",
    "        .withWatermark(\"timestamp\", \"1 minutes\") \\\n",
    "        .groupby(\"date_only\",\n",
    "                 \"date_hour\", \n",
    "                 \"date_minute\",\"timestamp\")\\\n",
    "        .agg(fn.approx_count_distinct('review_id').alias('total_reviews')\n",
    "             ,fn.approx_count_distinct('business_id').alias('total_business')\n",
    "             ,fn.approx_count_distinct('user_id').alias('total_users') ).alias(\"data\").select(\"data.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "831ff092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/18 13:09:04 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-67b3c69b-f2b5-41df-ad54-0d3c826b59ce. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "21/08/18 13:09:06 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+---------+-----------+---------+-------------+--------------+-----------+\n",
      "|date_only|date_hour|date_minute|timestamp|total_reviews|total_business|total_users|\n",
      "+---------+---------+-----------+---------+-------------+--------------+-----------+\n",
      "+---------+---------+-----------+---------+-------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#console print for debuging \n",
    "event_message_detail_agg_df_stream = event_message_detail_agg_df \\\n",
    "        .writeStream \\\n",
    "        .trigger(processingTime='60 seconds') \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .option(\"truncate\", \"false\")\\\n",
    "        .format(\"console\") \\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6244615",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "https://docs.databricks.com/spark/latest/structured-streaming/foreach.html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9724d6",
   "metadata": {},
   "source": [
    "My sql "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c24f8a",
   "metadata": {},
   "source": [
    "#write to sql database \n",
    "from pyspark.sql import DataFrameWriter\n",
    "\n",
    "db_target_properties = {\n",
    "    'user': \"root\",\n",
    "    'password': \"mypass\",\n",
    "    'driver': 'com.mysql.jdbc.Driver',\n",
    "    'useSSL':'false'\n",
    "}\n",
    "\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    df.show()\n",
    "    df.write.jdbc(url='jdbc:mysql://185.185.126.143:5340/Yelp_Test', mode='append',  table=\"all_data3\",  properties=db_target_properties)\n",
    "    pass\n",
    "\n",
    "#query = df2.writeStream.outputMode(\"append\").foreachBatch(foreach_batch_function).start() \n",
    "event_message_detail_agg_df_stream1 = event_message_detail_agg_df \\\n",
    "        .writeStream \\\n",
    "        .trigger(processingTime='60 seconds') \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .option(\"truncate\", \"false\").foreachBatch(foreach_batch_function).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb6352",
   "metadata": {},
   "source": [
    "Cassandra \n",
    "\n",
    "In my case the section below needed to be restarted and run many times , while the producer is still running , to fully populate the table\n",
    "This is due to RAM issues on my system.\n",
    "Finally the table populated with 1,084,335 reviews and read the complete data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac329420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/18 10:37:41 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-62bf691d-b912-4df9-a596-2d0761bf9166. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f34706a2c40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+--------------------+-------------+--------------+-----------+\n",
      "| date_only|date_hour|date_minute|           timestamp|total_reviews|total_business|total_users|\n",
      "+----------+---------+-----------+--------------------+-------------+--------------+-----------+\n",
      "|2018-04-05|        1|         46|2021-08-18 10:37:...|            2|             2|          2|\n",
      "|2018-04-05|        1|         51|2021-08-18 10:37:...|            5|             5|          5|\n",
      "|2018-04-05|       11|         33|2021-08-18 10:37:...|            1|             1|          1|\n",
      "|2018-04-05|       13|         22|2021-08-18 10:37:...|            1|             1|          1|\n",
      "|2018-04-05|       20|         41|2021-08-18 10:37:...|            2|             2|          2|\n",
      "|2018-04-05|       21|         15|2021-08-18 10:37:...|            3|             3|          3|\n",
      "|2018-04-05|        3|         16|2021-08-18 10:37:...|            2|             2|          2|\n",
      "|2018-04-05|        5|         14|2021-08-18 10:37:...|            1|             1|          1|\n",
      "|2018-04-05|       11|         42|2021-08-18 10:37:...|            2|             2|          2|\n",
      "|2018-04-05|       13|         57|2021-08-18 10:37:...|            3|             3|          3|\n",
      "|2018-04-05|        0|         14|2021-08-18 10:37:...|            5|             5|          5|\n",
      "|2018-04-05|        3|         12|2021-08-18 10:37:...|            2|             2|          2|\n",
      "|2018-04-05|       13|         37|2021-08-18 10:37:...|            3|             3|          3|\n",
      "|2018-04-05|       14|         37|2021-08-18 10:37:...|            1|             1|          1|\n",
      "|2018-04-05|       16|          3|2021-08-18 10:37:...|            1|             1|          1|\n",
      "|2018-04-05|       16|          9|2021-08-18 10:37:...|            4|             4|          4|\n",
      "|2018-04-05|       19|         35|2021-08-18 10:37:...|            4|             4|          4|\n",
      "|2018-04-05|       20|         14|2021-08-18 10:37:...|            2|             2|          2|\n",
      "|2018-04-05|        4|          0|2021-08-18 10:37:...|            1|             1|          1|\n",
      "|2018-04-05|       15|         48|2021-08-18 10:37:...|            1|             1|          1|\n",
      "+----------+---------+-----------+--------------------+-------------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/18 10:38:56 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 60000 milliseconds, but spent 74609 milliseconds\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----------+---------+-----------+--------------------+-------------+--------------+-----------+\n",
      "| date_only|date_hour|date_minute|           timestamp|total_reviews|total_business|total_users|\n",
      "+----------+---------+-----------+--------------------+-------------+--------------+-----------+\n",
      "|2018-01-01|        0|          3|2021-08-18 10:38:...|            6|             6|          6|\n",
      "|2018-01-01|        2|         18|2021-08-18 10:38:...|            3|             3|          3|\n",
      "|2018-01-01|       21|          5|2021-08-18 10:38:...|            3|             3|          3|\n",
      "|2018-01-01|       23|          9|2021-08-18 10:38:...|            2|             2|          2|\n",
      "|2018-01-02|        0|         34|2021-08-18 10:38:...|            2|             2|          2|\n",
      "|2018-01-02|        1|         14|2021-08-18 10:38:...|            2|             2|          2|\n",
      "|2018-01-02|        2|         34|2021-08-18 10:38:...|            3|             3|          3|\n",
      "|2018-01-02|       16|         11|2021-08-18 10:38:...|            1|             1|          1|\n",
      "|2018-01-02|       20|         14|2021-08-18 10:38:...|            4|             4|          4|\n",
      "|2018-01-02|       23|         11|2021-08-18 10:38:...|            3|             3|          3|\n",
      "|2018-01-03|        2|         39|2021-08-18 10:38:...|            2|             2|          2|\n",
      "|2018-01-03|        7|         16|2021-08-18 10:38:...|            1|             1|          1|\n",
      "|2018-01-03|       15|         58|2021-08-18 10:38:...|            3|             3|          3|\n",
      "|2018-01-04|        1|         14|2021-08-18 10:38:...|            3|             3|          3|\n",
      "|2018-01-04|        2|         45|2021-08-18 10:38:...|            2|             2|          2|\n",
      "|2018-01-04|        4|         46|2021-08-18 10:38:...|            1|             1|          1|\n",
      "|2018-01-04|        9|         59|2021-08-18 10:38:...|            1|             1|          1|\n",
      "|2018-01-04|       11|         58|2021-08-18 10:38:...|            1|             1|          1|\n",
      "|2018-01-04|       14|         49|2021-08-18 10:38:...|            2|             2|          2|\n",
      "|2018-01-04|       17|         17|2021-08-18 10:38:...|            2|             2|          2|\n",
      "+----------+---------+-----------+--------------------+-------------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:>                                                         (0 + 1) / 1]\r",
      "\r",
      "[Stage 24:>                 (0 + 1) / 1][Stage 27:>               (0 + 0) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-----------------------+-------------+--------------+-----------+\n",
      "|date_only |date_hour|date_minute|timestamp              |total_reviews|total_business|total_users|\n",
      "+----------+---------+-----------+-----------------------+-------------+--------------+-----------+\n",
      "|2018-01-01|9        |9          |2021-08-18 10:38:00.022|1            |1             |1          |\n",
      "|2018-01-01|19       |4          |2021-08-18 10:38:00.022|3            |3             |3          |\n",
      "|2018-01-01|22       |44         |2021-08-18 10:38:00.022|6            |6             |6          |\n",
      "|2018-01-02|4        |29         |2021-08-18 10:38:00.022|2            |2             |2          |\n",
      "|2018-01-02|6        |24         |2021-08-18 10:38:00.022|1            |1             |1          |\n",
      "|2018-01-02|14       |7          |2021-08-18 10:38:00.022|2            |2             |2          |\n",
      "|2018-01-02|15       |45         |2021-08-18 10:38:00.022|2            |2             |2          |\n",
      "|2018-01-02|18       |53         |2021-08-18 10:38:00.022|4            |4             |4          |\n",
      "|2018-01-03|3        |53         |2021-08-18 10:38:00.022|3            |3             |3          |\n",
      "|2018-01-03|5        |39         |2021-08-18 10:38:00.022|1            |1             |1          |\n",
      "|2018-01-03|12       |22         |2021-08-18 10:38:00.022|1            |1             |1          |\n",
      "|2018-01-03|16       |22         |2021-08-18 10:38:00.022|2            |2             |2          |\n",
      "|2018-01-03|19       |30         |2021-08-18 10:38:00.022|5            |5             |5          |\n",
      "|2018-01-03|22       |25         |2021-08-18 10:38:00.022|3            |3             |3          |\n",
      "|2018-01-04|5        |5          |2021-08-18 10:38:00.022|3            |3             |3          |\n",
      "|2018-01-04|5        |42         |2021-08-18 10:38:00.022|1            |1             |1          |\n",
      "|2018-01-04|10       |0          |2021-08-18 10:38:00.022|1            |1             |1          |\n",
      "|2018-01-04|12       |13         |2021-08-18 10:38:00.022|1            |1             |1          |\n",
      "|2018-01-04|17       |41         |2021-08-18 10:38:00.022|2            |2             |2          |\n",
      "|2018-01-04|22       |41         |2021-08-18 10:38:00.022|2            |2             |2          |\n",
      "+----------+---------+-----------+-----------------------+-------------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/18 10:41:44 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 60000 milliseconds, but spent 224717 milliseconds\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+--------------------+-------------+--------------+-----------+\n",
      "| date_only|date_hour|date_minute|           timestamp|total_reviews|total_business|total_users|\n",
      "+----------+---------+-----------+--------------------+-------------+--------------+-----------+\n",
      "|2018-04-05|       22|         53|2021-08-18 10:38:...|            3|             3|          3|\n",
      "|2018-04-06|        0|         51|2021-08-18 10:38:...|            3|             3|          3|\n",
      "|2018-04-06|        1|         56|2021-08-18 10:38:...|            4|             4|          4|\n",
      "|2018-04-06|        2|         27|2021-08-18 10:38:...|            4|             4|          4|\n",
      "|2018-04-06|        4|         35|2021-08-18 10:38:...|            2|             2|          2|\n",
      "|2018-04-06|       12|          0|2021-08-18 10:38:...|            2|             2|          2|\n",
      "|2018-04-06|       13|         55|2021-08-18 10:38:...|            1|             1|          1|\n",
      "|2018-04-06|       16|         11|2021-08-18 10:38:...|            4|             4|          4|\n",
      "|2018-04-06|       17|         32|2021-08-18 10:38:...|            2|             2|          2|\n",
      "|2018-04-06|       21|         15|2021-08-18 10:38:...|            5|             5|          5|\n",
      "|2018-04-06|       22|          6|2021-08-18 10:38:...|            2|             2|          2|\n",
      "|2018-04-06|       23|         28|2021-08-18 10:38:...|            3|             3|          3|\n",
      "|2018-04-07|        5|         39|2021-08-18 10:38:...|            2|             2|          2|\n",
      "|2018-04-07|        6|         19|2021-08-18 10:38:...|            2|             2|          2|\n",
      "|2018-04-07|        7|          1|2021-08-18 10:38:...|            1|             1|          1|\n",
      "|2018-04-07|       12|         16|2021-08-18 10:38:...|            1|             1|          1|\n",
      "|2018-04-07|       17|         43|2021-08-18 10:38:...|            3|             3|          3|\n",
      "|2018-04-07|       21|         46|2021-08-18 10:38:...|            6|             6|          6|\n",
      "|2018-04-08|        3|         11|2021-08-18 10:38:...|            5|             5|          5|\n",
      "|2018-04-08|        3|         23|2021-08-18 10:38:...|            1|             1|          1|\n",
      "+----------+---------+-----------+--------------------+-------------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/18 10:44:08 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 60000 milliseconds, but spent 368997 milliseconds\n",
      "21/08/18 10:44:40 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:42 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:43 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:45 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:46 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:46 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:47 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:48 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:49 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:49 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:50 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:51 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:51 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:52 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:53 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:53 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:54 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:44:55 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n",
      "21/08/18 10:45:16 WARN TaskMemoryManager: Failed to allocate a page (16777216 bytes), try again.\n"
     ]
    }
   ],
   "source": [
    "#write to sql database \n",
    "from pyspark.sql import DataFrameWriter\n",
    "\n",
    "\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    df.show()\n",
    "    #df.write.jdbc(url='jdbc:mysql://185.185.126.143:5340/Yelp_Test', mode='append',  table=\"all_data3\",  properties=db_target_properties)\n",
    "    df.write.format(\"org.apache.spark.sql.cassandra\").mode('append').options(table=\"review_summary_2\", keyspace=\"yelp\").save()\n",
    "    pass\n",
    "\n",
    "#query = df2.writeStream.outputMode(\"append\").foreachBatch(foreach_batch_function).start() \n",
    "#event_message_detail_agg_df_stream1 = \n",
    "event_message_detail_agg_df \\\n",
    "        .writeStream \\\n",
    "        .trigger(processingTime='60 seconds') \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .option(\"truncate\", \"false\").foreachBatch(foreach_batch_function).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d68f470",
   "metadata": {},
   "source": [
    "## 3.1 Alerts A : A consumer posts more than four posts in a rolling minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7db511d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/18 14:44:00 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9b6733fe-bf76-4471-aa28-aa23a284b624. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "queryStreamMem4 = (df2\n",
    " .writeStream\n",
    " .format(\"memory\")\n",
    " .queryName(\"base5\") #name for this query\n",
    " .outputMode(\"update\")\n",
    " .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186365ff",
   "metadata": {},
   "source": [
    "# Option a : sql method "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee3f58",
   "metadata": {},
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "\n",
    " #Count rows every 5 seconds while stream is active\n",
    "try:\n",
    "    i=1\n",
    "    # While stream is active, print count\n",
    "    while len(spark.streams.active) > 0:\n",
    "        \n",
    "        # Clear output\n",
    "        clear_output(wait=True)\n",
    "        print(\"Run:{}\".format(i))\n",
    "        \n",
    "        lst_queries = []\n",
    "        for s in spark.streams.active:\n",
    "            lst_queries.append(s.name)\n",
    "\n",
    "        # Verify if wiki_changes_count query is active before count\n",
    "        if \"base5\" in lst_queries:\n",
    "            # Count number of events\n",
    "             #spark.sql(\"select count(1) as Total_Posts from base\").show()\n",
    "            spark.sql(\"select user_id , count(distinct review_id) as Total_Posts from base5 group by user_id having Total_Posts>=4 \").show()\n",
    "        else:\n",
    "            print(\"'base5' query not found.\")\n",
    "\n",
    "        sleep(2)      #Report every 10 seconds \n",
    "        i=i+1\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    # Stop Query Stream\n",
    "    queryStreamMem.stop()\n",
    "    \n",
    "    print(\"stream process interrupted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d326af3",
   "metadata": {},
   "source": [
    "## Option b Spark windows : Sliding windows with watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e871bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "windowedCountsDF = \\\n",
    "     df2 \\\n",
    "    .groupBy(\n",
    "      \"user_id\",\n",
    "      window(\"Time_Only\", \"1 minutes\", \"1 minutes\")) \\\n",
    "     .count() \\\n",
    "     .where(\"count > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7b8c768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/18 14:50:53 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a739a79d-8a94-4310-9d9a-babf42c1c46e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "[Stage 28:=====================================================>(197 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+------+-----+\n",
      "|user_id|window|count|\n",
      "+-------+------+-----+\n",
      "+-------+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#console print for debuging \n",
    "windowedCountsDF_stream = windowedCountsDF \\\n",
    "        .writeStream \\\n",
    "        .trigger(processingTime='10 seconds') \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .option(\"truncate\", \"false\")\\\n",
    "        .format(\"console\") \\\n",
    "        .start() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c876fe1d",
   "metadata": {},
   "source": [
    "## 3.2 Alert B : A business gets more than 5 posts in a rolling minute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "653d9848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "windowedCountsDF_2 = \\\n",
    "     df2 \\\n",
    "    .groupBy(\n",
    "      \"business_id\",\n",
    "      window(\"Time_Only\", \"5 minutes\", \"1 minutes\")) \\\n",
    "     .count() \\\n",
    "     .where(\"count > 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e137d985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/18 14:43:27 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-928b23c4-b36b-41f9-bbda-5ef02bfaf27f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+------+-----+\n",
      "|user_id|window|count|\n",
      "+-------+------+-----+\n",
      "+-------+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------+------+-----+\n",
      "|business_id|window|count|\n",
      "+-----------+------+-----+\n",
      "+-----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#console print for debuging \n",
    "windowedCountsDF_stream = windowedCountsDF_2 \\\n",
    "        .writeStream \\\n",
    "        .trigger(processingTime='60 seconds') \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .option(\"truncate\", \"false\")\\\n",
    "        .format(\"console\") \\\n",
    "        .start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a617c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
